{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset Analysis\n",
    "\n",
    "This notebook demonstrates the application of various machine learning algorithms to the Titanic dataset. Each section includes a brief explanation of the algorithm and the corresponding code.\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "First, we load and preprocess the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId       0\n",
      "Survived          0\n",
      "Pclass            0\n",
      "Name              0\n",
      "Sex               0\n",
      "Age               0\n",
      "SibSp             0\n",
      "Parch             0\n",
      "Ticket            0\n",
      "Fare              0\n",
      "Cabin           687\n",
      "Embarked          0\n",
      "FamilySize        0\n",
      "IsAlone           0\n",
      "FareCategory      0\n",
      "Title             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "file_path = 'C:/Users/Me/Downloads/titanic.csv'  # Update this with the actual path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a new feature FamilySize\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# Create a new binary feature IsAlone\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Convert Fare to a categorical feature\n",
    "df['FareCategory'] = pd.qcut(df['Fare'], 3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Extract Title from Name\n",
    "df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Handle missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Prepare the data for modeling\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked', 'FareCategory', 'Title'], drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "features = df.drop(columns=['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'])\n",
    "X = features\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "Logistic regression is used for binary classification problems. It uses the sigmoid function to map predicted values to probabilities. The cost function is optimized using gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.8156424581005587, Precision: 0.7808219178082192, Recall: 0.7702702702702703, F1-Score: 0.7755102040816326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression with increased max_iter\n",
    "logistic_model = LogisticRegression(max_iter=500)\n",
    "accuracy, precision, recall, f1 = evaluate_model(logistic_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "print(f'Logistic Regression - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree\n",
    "\n",
    "Decision trees are used for both regression and classification tasks. They split the data into subsets based on feature values, creating a tree-like model. The splits are chosen to maximize information gain or minimize impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy: 0.776536312849162, Precision: 0.717948717948718, Recall: 0.7567567567567568, F1-Score: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "accuracy, precision, recall, f1 = evaluate_model(decision_tree_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'Decision Tree - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest\n",
    "\n",
    "Random forest is an ensemble method that combines multiple decision trees. It reduces overfitting by averaging the predictions of individual trees. It is robust and provides high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.8379888268156425, Precision: 0.7922077922077922, Recall: 0.8243243243243243, F1-Score: 0.8079470198675497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest\n",
    "random_forest_model = RandomForestClassifier()\n",
    "accuracy, precision, recall, f1 = evaluate_model(random_forest_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'Random Forest - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machine (SVM)\n",
    "\n",
    "SVM is used for classification tasks. It finds the hyperplane that best separates the classes in the feature space. It can handle non-linear data using kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine - Accuracy: 0.664804469273743, Precision: 0.7692307692307693, Recall: 0.2702702702702703, F1-Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVC()\n",
    "accuracy, precision, recall, f1 = evaluate_model(svm_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'Support Vector Machine - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is a simple, instance-based learning algorithm. It classifies a data point based on the majority class of its k-nearest neighbors. It is easy to implement but can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors - Accuracy: 0.7206703910614525, Precision: 0.7142857142857143, Recall: 0.5405405405405406, F1-Score: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "accuracy, precision, recall, f1 = evaluate_model(knn_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'K-Nearest Neighbors - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem. It assumes independence between features. It is fast and works well with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Accuracy: 0.664804469273743, Precision: 1.0, Recall: 0.1891891891891892, F1-Score: 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Naive Bayes\n",
    "naive_bayes_model = GaussianNB()\n",
    "accuracy, precision, recall, f1 = evaluate_model(naive_bayes_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'Naive Bayes - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gradient Boosting\n",
    "\n",
    "Gradient boosting is an ensemble method that builds models sequentially. Each new model corrects the errors of the previous ones. It is powerful but can be prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - Accuracy: 0.8100558659217877, Precision: 0.7941176470588235, Recall: 0.7297297297297297, F1-Score: 0.7605633802816901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting\n",
    "gradient_boosting_model = GradientBoostingClassifier()\n",
    "accuracy, precision, recall, f1 = evaluate_model(gradient_boosting_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'Gradient Boosting - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. XGBoost\n",
    "\n",
    "XGBoost is an optimized implementation of gradient boosting. It includes regularization to prevent overfitting. It is highly efficient and widely used in competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# XGBoost\n",
    "xgboost_model = XGBClassifier()\n",
    "accuracy, precision, recall, f1 = evaluate_model(xgboost_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f'XGBoost - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
